{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "d100d5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "95691410",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We continue form what we built last lecture\n",
    "# read in all the words\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "words[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "a2fca642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "# build the vocabulary of characters and mappings to/from integers\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "vocab_size = len(itos)\n",
    "print(itos)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "5be72cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182625, 3]) torch.Size([182625])\n",
      "torch.Size([22655, 3]) torch.Size([22655])\n",
      "torch.Size([22866, 3]) torch.Size([22866])\n"
     ]
    }
   ],
   "source": [
    "# build the dataset\n",
    "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
    "\n",
    "def build_dataset(words):  \n",
    "    X, Y = [], []\n",
    "  \n",
    "    for w in words:\n",
    "        context = [0] * block_size\n",
    "        for ch in w + '.':\n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            context = context[1:] + [ix] # crop and append\n",
    "\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    print(X.shape, Y.shape)\n",
    "    return X, Y\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr,  Ytr  = build_dataset(words[:n1])     # 80%\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])   # 10%\n",
    "Xte,  Yte  = build_dataset(words[n2:])     # 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "244e676f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11897\n"
     ]
    }
   ],
   "source": [
    "# MLP revisited (same we saw but without the input magic numbers)\n",
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 200 # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g)\n",
    "b1 = torch.randn(n_hidden,                        generator=g)\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g)\n",
    "b2 = torch.randn(vocab_size,                      generator=g)\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "32033252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0/ 200000: 27.8817\n",
      "  10000/ 200000: 2.7906\n",
      "  20000/ 200000: 2.6104\n",
      "  30000/ 200000: 2.8584\n",
      "  40000/ 200000: 2.0437\n",
      "  50000/ 200000: 2.5741\n",
      "  60000/ 200000: 2.3921\n",
      "  70000/ 200000: 2.1090\n",
      "  80000/ 200000: 2.2418\n",
      "  90000/ 200000: 2.3457\n",
      " 100000/ 200000: 2.0623\n",
      " 110000/ 200000: 2.3801\n",
      " 120000/ 200000: 2.1086\n",
      " 130000/ 200000: 2.4269\n",
      " 140000/ 200000: 2.1813\n",
      " 150000/ 200000: 2.1802\n",
      " 160000/ 200000: 2.0950\n",
      " 170000/ 200000: 1.8144\n",
      " 180000/ 200000: 2.0726\n",
      " 190000/ 200000: 1.9140\n"
     ]
    }
   ],
   "source": [
    "# And we do the same optimization as last time\n",
    "max_steps = 200000 # optimized number of steps\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "\n",
    "for i in range(max_steps):\n",
    "    # minibatch construct\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "    Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
    "    \n",
    "    # forward pass\n",
    "    emb = C[Xb] # embed the characters into vectors\n",
    "    embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "    hpreact = embcat @ W1 + b1 # hidden pre-activation\n",
    "    h = torch.tanh(hpreact) # hidden layer\n",
    "    logits = h @ W2 + b2 #output layer\n",
    "    loss = F.cross_entropy(logits, Yb) # loss function\n",
    "    \n",
    "    # backwards pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    \n",
    "    # update\n",
    "    lr = 0.1 if i < 100000 else 0.01 #stop learning rate decay\n",
    "    for p in parameters:\n",
    "        p.data += -lr*p.grad\n",
    "        \n",
    "    # track stats\n",
    "    if i % 10000 == 0: # print every once in a while\n",
    "        print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "    lossi.append(loss.log10().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "5b6564dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x18c1993fe80>]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# we see again the loss\n",
    "plt.plot(lossi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "dcd39357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 2.1250383853912354\n",
      "val 2.1684672832489014\n"
     ]
    }
   ],
   "source": [
    "# we compare the train loss and the val loss\n",
    "@torch.no_grad() # this decorator disables gradient tracking\n",
    "def split_loss(split):\n",
    "    x, y = {\n",
    "        'train': (Xtr, Ytr),\n",
    "        'val': (Xdev, Ydev),\n",
    "        'test':(Xte, Yte)\n",
    "    }[split]\n",
    "    emb = C[x]\n",
    "    embcat = emb.view(emb.shape[0], -1) # concat into(N, block_size*n_embed)\n",
    "    hpreact = embcat @ W1 + b1\n",
    "    h = torch.tanh(hpreact) # (N, n_hidden)\n",
    "    logits = h @ W2 + b2 # (N, vocab_size)\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "    print(split, loss.item())\n",
    "\n",
    "split_loss('train')\n",
    "split_loss('val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "81338056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chawry.\n",
      "gir.\n",
      "ego.\n",
      "dazabeus.\n",
      "josefynn.\n",
      "zamarethy.\n",
      "raniyan.\n",
      "nylaylin.\n",
      "kotana.\n",
      "aleah.\n"
     ]
    }
   ],
   "source": [
    "# and we finally sample from the model\n",
    "#g = torch.Generator().manual_seed(2147483647 + 10)\n",
    "\n",
    "for _ in range(10):\n",
    "    out = []\n",
    "    context = [0]*block_size\n",
    "    while True:\n",
    "        # forward pass the neural net\n",
    "        emb = C[torch.tensor([context])]\n",
    "        h = torch.tanh(emb.view(1, -1) @ W1 + b1)\n",
    "        logits = h @ W2 + b2\n",
    "        probs = F.softmax(logits, dim = 1)\n",
    "        \n",
    "        #sample from the distribution\n",
    "        ix = torch.multinomial(probs, num_samples = 1, generator = g).item()\n",
    "        \n",
    "        # shift the context window and track the samples\n",
    "        context = context[1:] + [ix]\n",
    "        out.append(ix)\n",
    "        \n",
    "        # if we sample the special '.' token we break\n",
    "        if ix == 0:\n",
    "            break\n",
    "        \n",
    "    # we print\n",
    "    print(''.join(itos[i] for i in out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e06345e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a49008c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.2958)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To start with this lecture we want to tackle the initialization, currently random\n",
    "# we do not expect loss of 27, but a way lower number\n",
    "# A first good guess for initi would be every characters proba to be uniform, 1/27\n",
    "iniloss = -torch.tensor(1/27).log()\n",
    "iniloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "efbe0cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12297\n"
     ]
    }
   ],
   "source": [
    "# This can be sorted out multiplying by 0 b2 and scaling down W2\n",
    "n_embd = 10\n",
    "n_hidden = 200\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * ((5/3)/((n_embd * block_size)**0.5))\n",
    "b1 = torch.randn(n_hidden,                        generator=g) * 0.01\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.01\n",
    "b2 = torch.randn(vocab_size,                      generator=g) * 0\n",
    "\n",
    "bngain = torch.ones((1, n_hidden))\n",
    "bnbias = torch.zeros((1, n_hidden))\n",
    "bnmean_running = torch.zeros((1, n_hidden))\n",
    "bnstd_running = torch.ones((1, n_hidden))\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "# Why not set W2 al to zero? We will see later, it has to do with the fact that we may not want them to be fully symmetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "248e1181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next problem arises when looking at the h\n",
    "#h # We have a lot of 1 and -1 values, the tanh is squashing the embeded inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d9edea94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# To visualize better\n",
    "plt.hist(h.view(-1).tolist(), 50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8b38d355",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This is because the preactivations inputed are very braod (-15 to 15)\n",
    "plt.hist(hpreact.view(-1).tolist(), 50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2ac404de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x18c23352b50>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# To solve this we see\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.imshow(h.abs() > 0.99, cmap = 'gray', interpolation = 'nearest')\n",
    "# We see a lot of white! meaning that the gradient that these neurons receive is close to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5c0587a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9990)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Therefore we want hpreact close to 0, b1 we will set low, and same for the weights W1\n",
    "# We see now a much more unspread gaussian\n",
    "# In general this is done using\n",
    "torch.randn(1000).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e79f71f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fan_in = n_embd * block_size\n",
    "wanted_std = (5/3)/(fan_in)**0.5\n",
    "# we incorporate this above, this function is also available in pytorch (init kaming normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "22f5b7bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 200])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We may also want to introduce a modelrn innovation in nn: batch normalization\n",
    "# The gist is basically instead of trying to construct hpreact as gaussian as possible (mean 0 std 1),\n",
    "# we can just normalize them into gaussian (differentiable operation and therefore gradient acceptable)\n",
    "hpreact.shape\n",
    "#hpreact.mean(0, keepdim = True) # mean across the 0th dimension (1, 200)\n",
    "#hpreact.std(0, keepdim = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d8382b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We include this in the optimization\n",
    "hpreact = (hpreact - hpreact.mean(0, keepdim = True))/hpreact.std(0, keepdim = True)\n",
    "# However, we only want to force gaussian (near) in the initialization\n",
    "# We do this by the 'scale and shift' (see batch norm paper), this is added in the parameters cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b6a2843f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0/ 200000: 3.3147\n",
      "  10000/ 200000: 2.1984\n",
      "  20000/ 200000: 2.3375\n",
      "  30000/ 200000: 2.4359\n",
      "  40000/ 200000: 2.0119\n",
      "  50000/ 200000: 2.2595\n",
      "  60000/ 200000: 2.4775\n",
      "  70000/ 200000: 2.1020\n",
      "  80000/ 200000: 2.2788\n",
      "  90000/ 200000: 2.1862\n",
      " 100000/ 200000: 1.9474\n",
      " 110000/ 200000: 2.3010\n",
      " 120000/ 200000: 1.9837\n",
      " 130000/ 200000: 2.4523\n",
      " 140000/ 200000: 2.3839\n",
      " 150000/ 200000: 2.1987\n",
      " 160000/ 200000: 1.9733\n",
      " 170000/ 200000: 1.8668\n",
      " 180000/ 200000: 1.9973\n",
      " 190000/ 200000: 1.8347\n"
     ]
    }
   ],
   "source": [
    "# And we do the same optimization as last time\n",
    "max_steps = 200000 # optimized number of steps\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "\n",
    "for i in range(max_steps):\n",
    "    # minibatch construct\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "    Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
    "    \n",
    "    # forward pass\n",
    "    emb = C[Xb] # embed the characters into vectors\n",
    "    embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "    hpreact = embcat @ W1 + b1 # hidden pre-activation\n",
    "    hpreact = bngain * (hpreact - hpreact.mean(0, keepdim = True))/hpreact.std(0, keepdim = True) + bnbias\n",
    "    h = torch.tanh(hpreact) # hidden layer\n",
    "    logits = h @ W2 + b2 #output layer\n",
    "    loss = F.cross_entropy(logits, Yb) # loss function\n",
    "    \n",
    "    # backwards pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    \n",
    "    # update\n",
    "    lr = 0.1 if i < 100000 else 0.01 #stop learning rate decay\n",
    "    for p in parameters:\n",
    "        p.data += -lr*p.grad\n",
    "        \n",
    "    # track stats\n",
    "    if i % 10000 == 0: # print every once in a while\n",
    "        print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "    lossi.append(loss.log10().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "05010cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 2.0668270587921143\n",
      "val 2.104844808578491\n"
     ]
    }
   ],
   "source": [
    "# We must also include the batch normalization idea to the test\n",
    "@torch.no_grad() # this decorator disables gradient tracking\n",
    "def split_loss(split):\n",
    "    x, y = {\n",
    "        'train': (Xtr, Ytr),\n",
    "        'val': (Xdev, Ydev),\n",
    "        'test':(Xte, Yte)\n",
    "    }[split]\n",
    "    emb = C[x]\n",
    "    embcat = emb.view(emb.shape[0], -1) # concat into(N, block_size*n_embed)\n",
    "    hpreact = embcat @ W1 + b1\n",
    "    hpreact = bngain * (hpreact - hpreact.mean(0, keepdim = True))/hpreact.std(0, keepdim = True) + bnbias\n",
    "    h = torch.tanh(hpreact) \n",
    "    logits = h @ W2 + b2 # (N, vocab_size)\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "    print(split, loss.item())\n",
    "\n",
    "split_loss('train')\n",
    "split_loss('val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ee576ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we work woth batches and the normaization forces us to have a distribuition of examples, not a single one.\n",
    "# We can get around that with a fixed mean and std of the entire training dataset\n",
    "with torch.no_grad():\n",
    "    # pass the training set through\n",
    "    emb = C[Xtr]\n",
    "    embcat = emb.view(emb.shape[0], -1)\n",
    "    hpreact = embcat @ W1 + b1\n",
    "    \n",
    "    # measure the mean/std over the entire training set\n",
    "    bnmean = hpreact.mean(0, keepdim = True)\n",
    "    bnstd = hpreact.std(0, keepdim = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "88054615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 2.0668270587921143\n",
      "val 2.1049270629882812\n"
     ]
    }
   ],
   "source": [
    "# Now, we add it into the split function\n",
    "@torch.no_grad() # this decorator disables gradient tracking\n",
    "def split_loss(split):\n",
    "    x, y = {\n",
    "        'train': (Xtr, Ytr),\n",
    "        'val': (Xdev, Ydev),\n",
    "        'test':(Xte, Yte)\n",
    "    }[split]\n",
    "    emb = C[x]\n",
    "    embcat = emb.view(emb.shape[0], -1) # concat into(N, block_size*n_embed)\n",
    "    hpreact = embcat @ W1 + b1\n",
    "    #hpreact = bngain * (hpreact - hpreact.mean(0, keepdim = True))/hpreact.std(0, keepdim = True) + bnbias\n",
    "    hpreact = bngain * (hpreact - bnmean)/bnstd + bnbias\n",
    "    h = torch.tanh(hpreact) \n",
    "    logits = h @ W2 + b2 # (N, vocab_size)\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "    print(split, loss.item())\n",
    "\n",
    "split_loss('train')\n",
    "split_loss('val')\n",
    "# And we can now also forward a single example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e7233748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12097\n"
     ]
    }
   ],
   "source": [
    "# We set again removing b1\n",
    "n_embd = 10\n",
    "n_hidden = 200\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * ((5/3)/((n_embd * block_size)**0.5))\n",
    "#b1 = torch.randn(n_hidden,                        generator=g) * 0.01\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.01\n",
    "b2 = torch.randn(vocab_size,                      generator=g) * 0\n",
    "\n",
    "bngain = torch.ones((1, n_hidden))\n",
    "bnbias = torch.zeros((1, n_hidden))\n",
    "bnmean_running = torch.zeros((1, n_hidden))\n",
    "bnstd_running = torch.ones((1, n_hidden))\n",
    "\n",
    "parameters = [C, W1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "# Why not set W2 al to zero? We will see later, it has to do with the fact that we may not want them to be fully symmetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "7c0aa154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0/ 200000: 3.3239\n",
      "  10000/ 200000: 2.0322\n",
      "  20000/ 200000: 2.5675\n",
      "  30000/ 200000: 2.0125\n",
      "  40000/ 200000: 2.2446\n",
      "  50000/ 200000: 1.8897\n",
      "  60000/ 200000: 2.0785\n",
      "  70000/ 200000: 2.3681\n",
      "  80000/ 200000: 2.2918\n",
      "  90000/ 200000: 2.0238\n",
      " 100000/ 200000: 2.3673\n",
      " 110000/ 200000: 2.3132\n",
      " 120000/ 200000: 1.6414\n",
      " 130000/ 200000: 1.9311\n",
      " 140000/ 200000: 2.2231\n",
      " 150000/ 200000: 2.0027\n",
      " 160000/ 200000: 2.0997\n",
      " 170000/ 200000: 2.4949\n",
      " 180000/ 200000: 2.0199\n",
      " 190000/ 200000: 2.1707\n"
     ]
    }
   ],
   "source": [
    "# Instead of having it fixed forever however, we will calculate it in each iteration of the training\n",
    "# And we do the same optimization as last time\n",
    "max_steps = 200000 # optimized number of steps\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "\n",
    "for i in range(max_steps):\n",
    "    # minibatch construct\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "    Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
    "    \n",
    "    # forward pass\n",
    "    emb = C[Xb] # embed the characters into vectors\n",
    "    embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "    # Linear layer\n",
    "    hpreact = embcat @ W1 #+ b1 # hidden pre-activation\n",
    "    # Batchnorm\n",
    "    bnmeani = hpreact.mean(0, keepdim = True)\n",
    "    bnstdi = hpreact.std(0, keepdim = True)\n",
    "    \n",
    "    # update mean and std\n",
    "    with torch.no_grad():\n",
    "        bnmean_running = 0.999 * bnmean + 0.001 * bnmeani\n",
    "        bnstd_running = 0.999 * bnstd + 0.001 * bnstdi\n",
    "    \n",
    "    hpreact = bngain * (hpreact - bnmeani)/bnstdi + bnbias\n",
    "    h = torch.tanh(hpreact) # hidden layer\n",
    "    logits = h @ W2 + b2 #output layer\n",
    "    loss = F.cross_entropy(logits, Yb) # loss function\n",
    "    \n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    \n",
    "    # update\n",
    "    lr = 0.1 if i < 100000 else 0.01 #stop learning rate decay\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "        \n",
    "    # track stats\n",
    "    if i % 10000 == 0: # print every once in a while\n",
    "        print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "    lossi.append(loss.log10().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "531d3a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 2.223217010498047\n",
      "val 2.266387701034546\n"
     ]
    }
   ],
   "source": [
    "# compare\n",
    "@torch.no_grad() # this decorator disables gradient tracking\n",
    "def split_loss(split):\n",
    "    x, y = {\n",
    "        'train': (Xtr, Ytr),\n",
    "        'val': (Xdev, Ydev),\n",
    "        'test':(Xte, Yte)\n",
    "    }[split]\n",
    "    emb = C[x]\n",
    "    embcat = emb.view(emb.shape[0], -1) # concat into(N, block_size*n_embed)\n",
    "    hpreact = embcat @ W1 + b1\n",
    "    #hpreact = bngain * (hpreact - hpreact.mean(0, keepdim = True))/hpreact.std(0, keepdim = True) + bnbias\n",
    "    hpreact = bngain * (hpreact - bnmean)/bnstd + bnbias\n",
    "    h = torch.tanh(hpreact) \n",
    "    logits = h @ W2 + b2 # (N, vocab_size)\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "    print(split, loss.item())\n",
    "\n",
    "split_loss('train')\n",
    "split_loss('val')\n",
    "# And we can now also forward a single example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "6aabb623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "18509734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last comments about batch normalization:\n",
    "# 1) Careful with the possible division by zero on hpreact, we may want to add a very small number there\n",
    "# 2) We do not need a bias (b1) before the batch normalization because it gets subtracted anywyas, so we comment it\n",
    "# Instead, we have not the batch normalization bias (bnbias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "3c42e4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "c5b2c04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can try to pytorchify the code (see video and jupyter notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60806af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
